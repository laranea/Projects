{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Traceback (most recent call last):\n  File \"/home/boo/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\n    from tensorflow.python.pywrap_tensorflow_internal import *\n  File \"/home/boo/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\n    _pywrap_tensorflow_internal = swig_import_helper()\n  File \"/home/boo/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\n  File \"/home/boo/anaconda3/envs/tf_gpu/lib/python3.6/imp.py\", line 243, in load_module\n    return load_dynamic(name, filename, file)\n  File \"/home/boo/anaconda3/envs/tf_gpu/lib/python3.6/imp.py\", line 343, in load_dynamic\n    return _load(spec)\nImportError: libnvidia-fatbinaryloader.so.384.130: cannot open shared object file: No such file or directory\n\n\nFailed to load the native TensorFlow runtime.\n\nSee https://www.tensorflow.org/install/errors\n\nfor some common reasons and solutions.  Include the entire stack trace\nabove this error message when asking for help.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m   \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpywrap_tensorflow_internal\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m   \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpywrap_tensorflow_internal\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_mod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0m_pywrap_tensorflow_internal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mswig_import_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mswig_import_helper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36mswig_import_helper\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m                 \u001b[0m_mod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_pywrap_tensorflow_internal'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.6/imp.py\u001b[0m in \u001b[0;36mload_module\u001b[0;34m(name, file, filename, details)\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mload_dynamic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mtype_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mPKG_DIRECTORY\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.6/imp.py\u001b[0m in \u001b[0;36mload_dynamic\u001b[0;34m(name, path, file)\u001b[0m\n\u001b[1;32m    342\u001b[0m             name=name, loader=loader, origin=path)\n\u001b[0;32m--> 343\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: libnvidia-fatbinaryloader.so.384.130: cannot open shared object file: No such file or directory",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a2b9ad62e5eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/tensorflow/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# pylint: disable=g-bad-import-order\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m  \u001b[0;31m# pylint: disable=unused-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/tensorflow/python/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcomponent_api_helper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msome\u001b[0m \u001b[0mcommon\u001b[0m \u001b[0mreasons\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msolutions\u001b[0m\u001b[0;34m.\u001b[0m  \u001b[0mInclude\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mentire\u001b[0m \u001b[0mstack\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m above this error message when asking for help.\"\"\" % traceback.format_exc()\n\u001b[0;32m---> 74\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;31m# pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: Traceback (most recent call last):\n  File \"/home/boo/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\n    from tensorflow.python.pywrap_tensorflow_internal import *\n  File \"/home/boo/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\n    _pywrap_tensorflow_internal = swig_import_helper()\n  File \"/home/boo/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\n  File \"/home/boo/anaconda3/envs/tf_gpu/lib/python3.6/imp.py\", line 243, in load_module\n    return load_dynamic(name, filename, file)\n  File \"/home/boo/anaconda3/envs/tf_gpu/lib/python3.6/imp.py\", line 343, in load_dynamic\n    return _load(spec)\nImportError: libnvidia-fatbinaryloader.so.384.130: cannot open shared object file: No such file or directory\n\n\nFailed to load the native TensorFlow runtime.\n\nSee https://www.tensorflow.org/install/errors\n\nfor some common reasons and solutions.  Include the entire stack trace\nabove this error message when asking for help."
     ]
    }
   ],
   "source": [
    "# %load ../import.py\n",
    "# import quandl\n",
    "# quandl.ApiConfig.api_key = 'MBMzvkxtv63KjFEV-tL6'\n",
    "# from quandl.errors.quandl_error import NotFoundError\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn import linear_model\n",
    "\n",
    "import ipywidgets as widgets \n",
    "from ipywidgets import  interact, interactive, fixed, interact_manual\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "# From tensorflow includes\n",
    "import math\n",
    "from IPython import display\n",
    "from matplotlib import cm\n",
    "from matplotlib import gridspec\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn import metrics\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\n",
    "    b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\n",
    "    c = tf.matmul(a, b)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print (sess.run(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "from IPython import display\n",
    "from matplotlib import cm\n",
    "from matplotlib import gridspec\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "pd.options.display.max_rows =10\n",
    "pd.options.display.float_format = '{:.1f}'.format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_data=pd.read_csv(\"https://storage.googleapis.com/mledu-datasets/california_housing_train.csv\", sep=\",\")\n",
    "\n",
    "# cal_data=cal_data.reindex(np.random.permutation(cal_data.index))\n",
    "cal_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output Dataset with this \n",
    "\n",
    "def preprocess_features(df):\n",
    "    \n",
    "    \"\"\"\n",
    "    Parameter:\n",
    "        Dataframe \n",
    "    Return:\n",
    "        Dataframe: with features only\n",
    "    \"\"\"\n",
    "    \n",
    "    selected_features = df[\n",
    "            [\n",
    "             'longitude',\n",
    "             'latitude',\n",
    "             'housing_median_age',\n",
    "             'total_rooms',\n",
    "             'total_bedrooms',\n",
    "             'population',\n",
    "             'households',\n",
    "             'median_income',\n",
    "            ]]\n",
    "    \n",
    "    processed_features = selected_features.copy()\n",
    "    processed_features[\"rooms_per_person\"] = (\n",
    "        df[\"total_rooms\"]/df[\"population\"]\n",
    "    )\n",
    "        \n",
    "    return processed_features\n",
    "\n",
    "def preprocess_targets(df):\n",
    "    \"\"\"\n",
    "    Parameter:\n",
    "        Dataframe:\n",
    "    Return:\n",
    "        Datafreme with target values only\n",
    "    \"\"\"\n",
    "    \n",
    "    output_targets=pd.DataFrame()\n",
    "    output_targets[\"median_house_value\"] = (\n",
    "        df[\"median_house_value\"]/1000.0)\n",
    "    \n",
    "    return output_targets\n",
    "\n",
    "# Output Feature Column\n",
    "\n",
    "def construct_feature_columns(input_features):\n",
    "    return set([\n",
    "        tf.feature_column.numeric_column(my_feature)\n",
    "        for my_feature in input_features \n",
    "        ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_examples   = preprocess_features(cal_data.head(12000))\n",
    "training_targets    = preprocess_targets( cal_data.head(12000))\n",
    "validation_examples = preprocess_features(cal_data.tail(5000))\n",
    "validation_targets  = preprocess_targets( cal_data.tail(5000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_data_cor=training_examples.copy()\n",
    "cal_data_cor[\"target\"]=training_targets[\"median_house_value\"]\n",
    "cal_data.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(13,8))\n",
    "\n",
    "ax =plt.subplot(1,2,1)\n",
    "ax.set_title(\"Validation\")\n",
    "\n",
    "\n",
    "ax.set_autoscaley_on(False)\n",
    "ax.set_ylim([32,43])\n",
    "ax.set_autoscalex_on(False)\n",
    "ax.set_xlim([-126,-112])\n",
    "\n",
    "plt.scatter(\n",
    "    validation_examples[\"longitude\"],\n",
    "    validation_examples[\"latitude\"],\n",
    "    cmap=\"coolwarm\",\n",
    "    c=validation_targets[\"median_house_value\"] / training_targets[\"median_house_value\"].max()\n",
    "    )\n",
    "\n",
    "\n",
    "ax =plt.subplot(1,2,2)\n",
    "ax.set_title(\"Training\")\n",
    "\n",
    "\n",
    "ax.set_autoscaley_on(False)\n",
    "ax.set_ylim([32,43])\n",
    "ax.set_autoscalex_on(False)\n",
    "ax.set_xlim([-126,-112])\n",
    "\n",
    "plt.scatter(\n",
    "    training_examples[\"longitude\"],\n",
    "    training_examples[\"latitude\"],\n",
    "    cmap=\"coolwarm\",\n",
    "    c=training_targets[\"median_house_value\"] / training_targets[\"median_house_value\"].max()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_input_fn(\n",
    "    features, \n",
    "    targets, \n",
    "    batch_size=1, \n",
    "    shuffle=True, \n",
    "    num_epochs=None):\n",
    "    \n",
    "        \"\"\" Parameters: \n",
    "        features: pd dataframe of feature\n",
    "        targets: pd dataframe of targets\n",
    "        batch_size: size of batch\n",
    "        shuffle shuffle data or not \n",
    "        num_epochs:  number of runs \n",
    "        \n",
    "        Returns:\n",
    "        Tuple of (features, lables) for next data batch\n",
    "        \"\"\"\n",
    "        \n",
    "        # Convert df into dict of np.array.\n",
    "        features = { \n",
    "                    key:np.array(value) \n",
    "                    for key, value in dict(features).items() \n",
    "                   }\n",
    "        \n",
    "        #Contruct a dataset, and configure batching/repaeating.\n",
    "        ds= Dataset.from_tensor_slices((features, targets))\n",
    "        ds= ds.batch(batch_size).repeat(num_epochs)\n",
    "        \n",
    "        # Shuffle if ordered\n",
    "        if shuffle:\n",
    "            ds =ds.shuffle(10000)\n",
    "            \n",
    "        # Return next batch of the data\n",
    "        features, labels =ds.make_one_shot_iterator().get_next()\n",
    "        \n",
    "        return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn_model(\n",
    "    learning_rate, \n",
    "    steps, \n",
    "    batch_size,\n",
    "    hidden_units,\n",
    "    training_examples,\n",
    "    training_targets,\n",
    "    validation_examples,\n",
    "    validation_targets):\n",
    "    \n",
    "    # How often you wnat updates:    \n",
    "    periods =10 \n",
    "    steps_per_period = steps/periods\n",
    "    \n",
    "    \"\"\" Parameters:\n",
    "    learning_rate: how big should our step should be \n",
    "    steps: how many times should we process batch\n",
    "    batch_size: sample size \n",
    "    input_feature: what should be the input \n",
    "    \"\"\"\n",
    "    \n",
    "    #Configure regressor:\n",
    "    my_optimizer= tf.train.AdamOptimizer(learning_rate= learning_rate)\n",
    "    my_optimizer= tf.contrib.estimator.clip_gradients_by_norm(my_optimizer, 5.0)\n",
    "    dnn_regressor = tf.estimator.DNNRegressor(\n",
    "        feature_columns=construct_feature_columns(training_examples),\n",
    "        hidden_units=hidden_units,\n",
    "        optimizer=my_optimizer\n",
    "        )\n",
    "    \n",
    "    # Creating input pfunction \n",
    "    \n",
    "    training_input_fn = lambda: my_input_fn(\n",
    "        training_examples,\n",
    "        training_targets[\"median_house_value\"],\n",
    "        batch_size=batch_size)\n",
    "    \n",
    "    predict_training_input_fn = lambda: my_input_fn(\n",
    "        training_examples,\n",
    "        training_targets[\"median_house_value\"],\n",
    "        num_epochs=1,\n",
    "        shuffle=False)\n",
    "    \n",
    "    predict_validation_input_fn = lambda: my_input_fn(\n",
    "        validation_examples,\n",
    "        validation_targets[\"median_house_value\"],\n",
    "        num_epochs=1,\n",
    "        shuffle=False)\n",
    "        \n",
    "\n",
    "    # Trainign model from below code\n",
    "    print(\"Training started:\")\n",
    "    print(\"RMSE on models are:\")\n",
    "    training_rmses=[]\n",
    "    validation_rmses=[]\n",
    "    \n",
    "    \n",
    "    for  period in  range(0, periods):\n",
    "        dnn_regressor.train(\n",
    "            input_fn = training_input_fn,\n",
    "            steps = steps_per_period\n",
    "            \n",
    "        )\n",
    "        \n",
    "    \n",
    "        # Take a break and and compute some predictions \n",
    "    \n",
    "        training_predictions =dnn_regressor.predict(input_fn=predict_training_input_fn)\n",
    "        training_predictions = np.array([\n",
    "                            item[\"predictions\"][0]\n",
    "                            for item in training_predictions \n",
    "                           ])\n",
    "    \n",
    "        validation_predictions =dnn_regressor.predict(input_fn=predict_validation_input_fn)\n",
    "        validation_predictions = np.array([\n",
    "                            item[\"predictions\"][0]\n",
    "                            for item in validation_predictions \n",
    "                           ])\n",
    "    \n",
    "    \n",
    "        # Compute loss\n",
    "        training_mse=metrics.mean_squared_error(training_predictions, training_targets)\n",
    "        training_rmse=math.sqrt( training_mse )\n",
    "        \n",
    "        validation_mse=metrics.mean_squared_error(validation_predictions, validation_targets)\n",
    "        validation_rmse=math.sqrt( validation_mse )\n",
    "        \n",
    "        \n",
    "        #Print current loss\n",
    "        print(\"period %02d: %0.2f\",period, training_rmse)\n",
    "        \n",
    "        #Save rmse\n",
    "        training_rmses.append(training_rmse)\n",
    "        validation_rmses.append(validation_rmse)\n",
    "       \n",
    "    print(\"Model training finished.\")\n",
    "    \n",
    "    # Output graph of loss metrics over periods. \n",
    "    plt.ylabel('RMSE')\n",
    "    plt.xlabel(\"Periods\")\n",
    "    plt.title(\"RMSE vs Periods\")\n",
    "    plt.tight_layout()\n",
    "    plt.plot(training_rmses, label=\"training\")\n",
    "    plt.plot(validation_rmses, label=\"validation\")\n",
    "    plt.legend()\n",
    "  \n",
    "\n",
    "    return dnn_regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Various function for Scaling\n",
    "def linear_scale(series):\n",
    "    min_val= series.min()\n",
    "    max_val= series.max()\n",
    "    scale = (max_val-min_val) / 2\n",
    "    return series.apply(lambda x: ((x-min_val)/scale) - 1.0)\n",
    "\n",
    "def log_scale(series):\n",
    "    return series.apply( lambda x:math.log(x+1.0))\n",
    "\n",
    "def clip(series):\n",
    "    return series.apply ( lambda x: (\n",
    "        min(max(x, clip_to_min), clip_to_max))\n",
    "        )\n",
    "\n",
    "def z_score(series):\n",
    "    mean=series.mean()\n",
    "    std_dv=series.std()\n",
    "    return series.apply(lambda x: (x-mean)/std_dv )\n",
    "    \n",
    "def normalize_scale(df,method):\n",
    "    processed_features= pd.DataFrame()\n",
    "    \n",
    "    target_list=[\"median_house_value\"]\n",
    "    for col in df.columns:\n",
    "        if col not in target_list: \n",
    "            processed_features[col] = method(df[col])\n",
    "        else:\n",
    "            pass\n",
    "    return processed_features\n",
    "\n",
    "def binary_treshold(series, threshold):\n",
    "    return series.apply(lambda x:(1 if x >threshold else 0))\n",
    "\n",
    "def clipper(series, threshold=20):\n",
    "    return series.apply(lambda x:(x if x < threshold else threshold))\n",
    "\n",
    "normal_cal_data= normalize_scale(preprocess_features(cal_data), clipper)\n",
    "normal_training_examples   = normal_cal_data.head(12000)\n",
    "normal_validation_examples = normal_cal_data.tail(5000)\n",
    "normal_cal_data\n",
    "\n",
    "_=normal_training_examples.hist(bins=20, figsize=(20,20), xlabelsize=10 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_regressor= train_nn_model(\n",
    "    learning_rate=1,\n",
    "    steps=2000,\n",
    "    batch_size=100,\n",
    "    hidden_units=[10,2],\n",
    "    training_examples=normal_training_examples,\n",
    "    training_targets=training_targets,\n",
    "    validation_examples=normal_validation_examples,\n",
    "    validation_targets=validation_targets\n",
    "    )\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_data=preprocess_features(cal_data).head(1)\n",
    "out_data=preprocess_targets(cal_data).head(1)\n",
    "out_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_training_input_fn = lambda: my_input_fn(\n",
    "        in_data,\n",
    "        out_data[\"median_house_value\"],\n",
    "        num_epochs=1,\n",
    "        shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_predict_tf = dnn_regressor.predict(input_fn = predict_training_input_fn)\n",
    "\n",
    "list(ds_predict_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.scatter(training_examples[\"median_income\"], training_targets[\"median_house_value\"])\n",
    "#plt.scatter(training_examples[\"longitude\"], training_targets[\"median_house_value\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/25119524/pandas-conditional-rolling-count\n",
    "    \n",
    "    \n",
    "#Set up plotting\n",
    "#     plt.figure(figsize=(15,6))\n",
    "#     plt.subplot(1,2,1)\n",
    "#     plt.title(\" Learnd line period\")\n",
    "#     plt.ylabel(my_label)\n",
    "#     plt.xlabel(my_feature)\n",
    "    \n",
    "#     sample=df.sample(n=300)\n",
    "#     plt.scatter(sample[my_feature], sample[my_label])\n",
    "#     colors = [cm.coolwarm(x) \n",
    "#              for x in np.linspace(-1, 1, periods)\n",
    "#              ]\n",
    "    \n",
    "    \n",
    "\n",
    "#         # PLot weights and bias using graph \n",
    "#         y_extents=np.array([  0, sample[my_label].max()   ])\n",
    "\n",
    "#         weight = linear_regressor.get_variable_value('linear/linear_model/%s/weights' % input_feature)[0]\n",
    "#         bias = linear_regressor.get_variable_value('linear/linear_model/bias_weights')\n",
    "\n",
    "#         x_extents = (y_extents - bias) / weight\n",
    "#         x_extents = np.maximum(np.minimum(x_extents,\n",
    "#                                       sample[my_feature].max()),\n",
    "#                            sample[my_feature].min())\n",
    "#         y_extents = weight * x_extents + bias\n",
    "#         plt.plot(x_extents, y_extents, color=colors[period]) \n",
    "\n",
    "\n",
    "def binner(df, col, kitni_bar_katu):\n",
    "    max_of_col=df[col].max()\n",
    "    min_of_col=df[col].min()\n",
    "    range_of_col= max_of_col -min_of_col\n",
    "    \n",
    "    print(\"MAX:%0.2f\" % (max_of_col))\n",
    "    print(\"MIN:%0.2f\" % (min_of_col))\n",
    "\n",
    "    long_bin=[]\n",
    "    for i in range(0, 12):\n",
    "        long_bin.append(min_long)\n",
    "        min_long +=1\n",
    "\n",
    "pd.get_dummies(training_examples[\"longitude\"])\n",
    "pd.qcut(training_examples[\"longitude\"], 10, labels=False)\n",
    "\n",
    "# g=tf.one_hot(indices=training_examples[\"longitude\"], depth=10,on_value=1, off_value=0 )\n",
    "# g"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
